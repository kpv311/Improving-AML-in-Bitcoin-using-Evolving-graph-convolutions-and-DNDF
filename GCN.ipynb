{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Graph convolutional network.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ksc_Ogwxwwl2"
      },
      "source": [
        "## Pytorch Geometric Environment Setting"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LCidzPVYwxN9"
      },
      "source": [
        "# Install required packages.\n",
        "!pip install -q torch-scatter==latest+cu101 -f https://pytorch-geometric.com/whl/torch-1.7.0.html\n",
        "!pip install -q torch-sparse==latest+cu101 -f https://pytorch-geometric.com/whl/torch-1.7.0.html\n",
        "!pip install -q git+https://github.com/rusty1s/pytorch_geometric.git"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yvMENKuLwVvB"
      },
      "source": [
        "## Library Import"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8fFna02HwQxB"
      },
      "source": [
        "import numpy as np\n",
        "import networkx as nx\n",
        "import os\n",
        "import pandas as pd\n",
        "from sklearn.metrics import f1_score, precision_score, recall_score\n",
        "from sklearn.manifold import TSNE\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.nn import Embedding\n",
        "from torch.nn import Parameter\n",
        "from torch_geometric.data import Data,DataLoader\n",
        "from torch_geometric.nn import GCNConv\n",
        "from torch_geometric.utils.convert import to_networkx\n",
        "from torch_geometric.utils import to_undirected"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BLSPDBtLy-aT"
      },
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iot56tREpgpU"
      },
      "source": [
        "from google.colab import files\n",
        "files.upload()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TZLcvgEfiinM"
      },
      "source": [
        "\n",
        "os.environ['KAGGLE_USERNAME'] = \"karthikapv\" # username from the json file\n",
        "os.environ['KAGGLE_KEY'] = \"cc11b8fcbb2e177d31cd566bbabe382a\" # key from the json file\n",
        "!kaggle datasets download -d ellipticco/elliptic-data-set\n",
        "!unzip elliptic-data-set.zip\n",
        "!mkdir elliptic_bitcoin_dataset_cont"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wfVZ-ihYit3K"
      },
      "source": [
        "## Data Preparation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4q6mAv_0jNwD"
      },
      "source": [
        "# Load Dataframe\n",
        "df_edge = pd.read_csv('elliptic_bitcoin_dataset/elliptic_txs_edgelist.csv')\n",
        "df_class = pd.read_csv('elliptic_bitcoin_dataset/elliptic_txs_classes.csv')\n",
        "df_features = pd.read_csv('elliptic_bitcoin_dataset/elliptic_txs_features.csv',header=None)\n",
        "\n",
        "# Setting Column name\n",
        "df_features.columns = ['id', 'time step'] + [f'trans_feat_{i}' for i in range(93)] + [f'agg_feat_{i}' for i in range(72)]\n",
        "\n",
        "print('Number of edges: {}'.format(len(df_edge)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kqkMUFOmmJ9y"
      },
      "source": [
        "## Get Node Index"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bWoUAV16mLHS"
      },
      "source": [
        "all_nodes = list(set(df_edge['txId1']).union(set(df_edge['txId2'])).union(set(df_class['txId'])).union(set(df_features['id'])))\n",
        "nodes_df = pd.DataFrame(all_nodes,columns=['id']).reset_index()\n",
        "\n",
        "print('Number of nodes: {}'.format(len(nodes_df)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "03RoQLdPorg9"
      },
      "source": [
        "## Fix id index"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4dot3YJdotoU"
      },
      "source": [
        "df_edge = df_edge.join(nodes_df.rename(columns={'id':'txId1'}).set_index('txId1'),on='txId1',how='inner') \\\n",
        "       .join(nodes_df.rename(columns={'id':'txId2'}).set_index('txId2'),on='txId2',how='inner',rsuffix='2') \\\n",
        "       .drop(columns=['txId1','txId2']) \\\n",
        "       .rename(columns={'index':'txId1','index2':'txId2'})\n",
        "df_edge.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HGivrPVeo5Uy"
      },
      "source": [
        "df_class = df_class.join(nodes_df.rename(columns={'id':'txId'}).set_index('txId'),on='txId',how='inner') \\\n",
        "        .drop(columns=['txId']).rename(columns={'index':'txId'})[['txId','class']]\n",
        "df_class.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lpAjxw4VqprC"
      },
      "source": [
        "df_features = df_features.join(nodes_df.set_index('id'),on='id',how='inner') \\\n",
        "        .drop(columns=['id']).rename(columns={'index':'id'})\n",
        "df_features = df_features [ ['id']+list(df_features.drop(columns=['id']).columns) ]\n",
        "df_features.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qFFg89HyraP6"
      },
      "source": [
        "df_edge_time = df_edge.join(df_features[['id','time step']].rename(columns={'id':'txId1'}).set_index('txId1'),on='txId1',how='left',rsuffix='1') \\\n",
        ".join(df_features[['id','time step']].rename(columns={'id':'txId2'}).set_index('txId2'),on='txId2',how='left',rsuffix='2')\n",
        "df_edge_time['is_time_same'] = df_edge_time['time step'] == df_edge_time['time step2']\n",
        "df_edge_time_fin = df_edge_time[['txId1','txId2','time step']].rename(columns={'txId1':'source','txId2':'target','time step':'time'})"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3hLkqgWQtRoj"
      },
      "source": [
        "## Create csv from Dataframe"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0oRwz171sdF7"
      },
      "source": [
        "df_features.drop(columns=['time step']).to_csv('elliptic_bitcoin_dataset_cont/elliptic_txs_features.csv',index=False,header=None)\n",
        "df_class.rename(columns={'txId':'nid','class':'label'})[['nid','label']].sort_values(by='nid').to_csv('elliptic_bitcoin_dataset_cont/elliptic_txs_classes.csv',index=False,header=None)\n",
        "df_features[['id','time step']].rename(columns={'id':'nid','time step':'time'})[['nid','time']].sort_values(by='nid').to_csv('elliptic_bitcoin_dataset_cont/elliptic_txs_nodetime.csv',index=False,header=None)\n",
        "df_edge_time_fin[['source','target','time']].to_csv('elliptic_bitcoin_dataset_cont/elliptic_txs_edgelist_timed.csv',index=False,header=None)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "glU24wM7uuO6"
      },
      "source": [
        "## Graph Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ebolR4V10TaZ"
      },
      "source": [
        "node_label = df_class.rename(columns={'txId':'nid','class':'label'})[['nid','label']].sort_values(by='nid').merge(df_features[['id','time step']].rename(columns={'id':'nid','time step':'time'}),on='nid',how='left')\n",
        "node_label['label'] =  node_label['label'].apply(lambda x: '3'  if x =='unknown' else x).astype(int)-1\n",
        "node_label.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_f0dLVSrvjUD"
      },
      "source": [
        "merged_nodes_df = node_label.merge(df_features.rename(columns={'id':'nid','time step':'time'}).drop(columns=['time']),on='nid',how='left')\n",
        "merged_nodes_df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zquQgCXPsMIN"
      },
      "source": [
        "train_dataset=[]\n",
        "test_dataset=[]\n",
        "for i in range(49):\n",
        "    nodes_df_tmp=merged_nodes_df[merged_nodes_df['time']==i+1].reset_index()\n",
        "    nodes_df_tmp['index']=nodes_df_tmp.index\n",
        "    df_edge_tmp = df_edge_time_fin.join(nodes_df_tmp.rename(columns={'nid':'source'})[['source','index']].set_index('source'),on='source',how='inner')\\\n",
        "        .join(nodes_df_tmp.rename(columns={'nid':'target'})[['target','index']].set_index('target'),on='target',how='inner',rsuffix='2') \\\n",
        "        .drop(columns=['source','target']) \\\n",
        "        .rename(columns={'index':'source','index2':'target'})\n",
        "    x = torch.tensor(np.array(nodes_df_tmp.sort_values(by='index').drop(columns=['index','nid','label'])), dtype=torch.float)\n",
        "    edge_index = torch.tensor(np.array(df_edge_tmp[['source','target']]).T, dtype=torch.long)\n",
        "    edge_index = to_undirected(edge_index)\n",
        "    mask = nodes_df_tmp['label']!=2\n",
        "    y=torch.tensor(np.array(nodes_df_tmp['label']))\n",
        "\n",
        "    if i+1<35:\n",
        "        data = Data(x=x,edge_index=edge_index, train_mask=mask, y=y)\n",
        "        train_dataset.append(data)\n",
        "    else:\n",
        "        data = Data(x=x,edge_index=edge_index, test_mask=mask, y=y)\n",
        "        test_dataset.append(data)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=1, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=1, shuffle=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xHLjqRlXInHD"
      },
      "source": [
        "## Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h7tKytc-C4PT"
      },
      "source": [
        "class GCN(torch.nn.Module):\n",
        "    def __init__(self, num_node_features, hidden_channels, use_skip=False):\n",
        "        super(GCN, self).__init__()\n",
        "        self.conv1 = GCNConv(num_node_features, hidden_channels[0])\n",
        "        self.conv2 = GCNConv(hidden_channels[0], 2)\n",
        "        self.use_skip = use_skip\n",
        "        if self.use_skip:\n",
        "            self.weight = nn.init.xavier_normal_(Parameter(torch.Tensor(num_node_features, 2)))\n",
        "\n",
        "\n",
        "    def forward(self, data):\n",
        "        x = self.conv1(data.x, data.edge_index)\n",
        "        x = x.relu()\n",
        "        x = F.dropout(x, p=0.5, training=self.training)\n",
        "        x = self.conv2(x, data.edge_index)\n",
        "        if self.use_skip:\n",
        "            x = F.softmax(x+torch.matmul(data.x, self.weight), dim=-1)\n",
        "        else:\n",
        "            x = F.softmax(x, dim=-1)\n",
        "        return x\n",
        "\n",
        "    def embed(self, data):\n",
        "        x = self.conv1(data.x, data.edge_index)\n",
        "        return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZBQLzM2zKv55"
      },
      "source": [
        "model = GCN(num_node_features=data.num_node_features ,hidden_channels=[100])\n",
        "model.to(device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GM-0T8IJKA-m"
      },
      "source": [
        "## Train"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i96wwQ1RQ4vw"
      },
      "source": [
        "#### Hyperparameter"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y3JFJ6inQ4gK"
      },
      "source": [
        "patience = 50\n",
        "lr = 0.001\n",
        "epoches = 1000"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V5dxOv0KKCap"
      },
      "source": [
        "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
        "criterion = torch.nn.CrossEntropyLoss(weight=torch.tensor([0.7,0.3]).to(device))\n",
        "\n",
        "\n",
        "train_losses=[]\n",
        "val_losses =[]\n",
        "accuracies=[]\n",
        "if1=[]\n",
        "precisions=[]\n",
        "recalls=[]\n",
        "iterations=[]\n",
        "\n",
        "for epoch in range(epoches):\n",
        "    \n",
        "    model.train()\n",
        "    train_loss = 0\n",
        "    for data in train_loader:\n",
        "        data = data.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        out = model(data)\n",
        "        loss = criterion(out[data.train_mask], data.y[data.train_mask])\n",
        "        _, pred = out[data.train_mask].max(dim=1)\n",
        "        loss.backward()\n",
        "        train_loss += loss.item() * data.num_graphs\n",
        "        optimizer.step()\n",
        "    train_loss /= len(train_loader.dataset)\n",
        "    \n",
        "\n",
        "    if (epoch+1)%50==0:\n",
        "        model.eval()\n",
        "        ys, preds = [], []\n",
        "        val_loss = 0\n",
        "        for data in test_loader:\n",
        "            data = data.to(device)\n",
        "            out = model(data)\n",
        "            loss = criterion(out[data.test_mask], data.y[data.test_mask])\n",
        "            val_loss += loss.item() * data.num_graphs\n",
        "            _, pred = out[data.test_mask].max(dim=1)\n",
        "            ys.append(data.y[data.test_mask].cpu())\n",
        "            preds.append(pred.cpu())\n",
        "\n",
        "        y, pred = torch.cat(ys, dim=0).numpy(), torch.cat(preds, dim=0).numpy()\n",
        "        val_loss /= len(test_loader.dataset)\n",
        "        f1 = f1_score(y, pred, average=None)\n",
        "        mf1 = f1_score(y, pred, average='micro')\n",
        "        precision = precision_score(y, pred, average=None)\n",
        "        recall = recall_score(y, pred, average=None)\n",
        "\n",
        "        iterations.append(epoch+1)\n",
        "        train_losses.append(train_loss)\n",
        "        val_losses.append(val_loss)\n",
        "        if1.append(f1[0])\n",
        "        accuracies.append(mf1)\n",
        "        precisions.append(precision[0])\n",
        "        recalls.append(recall[0])\n",
        "\n",
        "        print('Epoch: {:02d}, Train_Loss: {:.4f}, Val_Loss: {:.4f}, Precision: {:.4f}, Recall: {:.4f}, Illicit f1: {:.4f}, F1: {:.4f}'.format(epoch+1, train_loss, val_loss, precision[0], recall[0], f1[0], mf1))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Adu5SekkqlTs"
      },
      "source": [
        "a,b,c,d = train_losses, val_losses, if1, accuracies\n",
        "plt.plot(np.array(a), 'r', label='Train loss')\n",
        "plt.plot(np.array(b), 'g', label='Valid loss')\n",
        "plt.plot(np.array(c), 'black', label='Illicit F1')\n",
        "plt.plot(np.array(d), 'orange', label='F1')\n",
        "plt.ylim([0,1.0])\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}